{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "carrie1_ecommerce_data_path = kagglehub.dataset_download('carrie1/ecommerce-data')\n",
        "vasopikof_clustering_benchmark_datasets_path = kagglehub.dataset_download('vasopikof/clustering-benchmark-datasets')\n",
        "hellbuoy_online_retail_customer_clustering_path = kagglehub.dataset_download('hellbuoy/online-retail-customer-clustering')\n",
        "harrywang_wine_dataset_for_clustering_path = kagglehub.dataset_download('harrywang/wine-dataset-for-clustering')\n",
        "sandeep2812_clustering_path = kagglehub.dataset_download('sandeep2812/clustering')\n",
        "joonasyoon_clustering_exercises_path = kagglehub.dataset_download('joonasyoon/clustering-exercises')\n",
        "huwfulcher_social_cluster_analysis_path = kagglehub.notebook_output_download('huwfulcher/social-cluster-analysis')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "bVYggMMuJsvl"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering methods - comprehensive study"
      ],
      "metadata": {
        "id": "7mxm8rwuJsvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![dsc_optimized.png](attachment:50fb7cd8-afb2-4315-9c15-58d131a083ff.png)"
      ],
      "metadata": {
        "id": "N65xN4xaJsvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "aDhyKImnJsvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering is one of the most frequently used forms of unsupervised learning. It automatically discover natural grouping in data.\n",
        "\n",
        "Clustering is especially useful for exploring data you know nothing about. You might find connections you never would have thought of. Clustering can also be useful as a type of feature engineering, where existing and new examples can be mapped and labeled as belonging to one of the identified clusters in the data.\n",
        "\n",
        "Some typical real world applications of clustering include fraud detection, categorizing books in a library or customer segmentation in marketing.\n"
      ],
      "metadata": {
        "id": "ev3I0wajJsvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "* 1. Types of clustering algorithms\n",
        "* 2. Set-up\n",
        "    * 2.1 Data sets\n",
        "    * 2.2 Import Libraries\n",
        "    * 2.3 Import Data\n",
        "    * 2.4 Some Visualisations\n",
        "    * 2.5 Feature engineering\n",
        "    * 2.6 Outlier detection\n",
        "    * 2.7 Scalling data\n",
        "* 3. Determining The Optimal Number Of Clusters\n",
        "    * 3.1 Elbow method\n",
        "    * 3.2 Silhouette method\n",
        "    * 3.3 Dendrogram\n",
        "* 4. K-Means\n",
        "    * 4.1 Advantages and disadvantages of K-Means\n",
        "    * 4.2 Variations of K-Means\n",
        "    * 4.3 Training the K-Means model on the datasets\n",
        "    * 4.4 Comparing results\n",
        "    * 4.5 K-Means on online retail data\n",
        "* 5. Hierarchical clustering\n",
        "    * 5.1 Advantages and Disadvantages of Hierarchical clustering\n",
        "    * 5.2 Variations of hierarchical clustering\n",
        "    * 5.3 Training the hierarchical clustering model on the datasets\n",
        "    * 5.4 Comparing results\n",
        "    * 5.5 Hierarchical clustering on online retail data\n",
        "* 6. DBSCAN clustering algorithm\n",
        "    * 6.1 Advantages and Disadvantages of DBSCAN\n",
        "    * 6.2 Choosing the right initial parameters\n",
        "    * 6.3 Variations of DBSCAN\n",
        "    * 6.4 Training of DBSCAN clustering model on the datasets\n",
        "    * 6.5 Comparing results\n",
        "    * 6.6 DBSCAN clustering model on online retail data\n",
        "* 7. Gaussian Mixture Models (GMM)\n",
        "    * 7.1 Advantages and Disadvantages of Gaussian Mixture Models\n",
        "    * 7.2 Variations of GMM\n",
        "    * 7.3 Training of GMM on the datasets\n",
        "    * 7.4 Comparing results\n",
        "    * 7.5 GMM clustering model on online retail data\n",
        "* 8. All algorithm comparison"
      ],
      "metadata": {
        "id": "6-sFhHa2Jsvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Types of clustering algorithms\n",
        "\n",
        "There are many clustering algorithms to choose. It is a good idea to explore a range of clustering algorithms and different configurations. It might take some time to figure out which type of clustering algorithm works the best for the given data, but when you do, you'll get invaluable insight on your data.\n",
        "\n",
        "**Centroid-based**\n",
        "\n",
        "* These types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid.\n",
        "\n",
        "* This is the most commonly used type of clustering. K-Means algorithm is one of the centroid based clustering algorithms. Here k is the number of clusters and is a hyperparameter to the algorithm.\n",
        "\n",
        "**Hierarchical-based (Connectivity-based)**\n",
        "\n",
        "* The idea is based on the core idea of objects being more related to nearby objects than to objects farther away.\n",
        "* It builds a tree of clusters so everything is organized from the top-down. Initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
        "\n",
        "**Density-based**\n",
        "\n",
        "* Data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\n",
        "\n",
        "* The clusters can be any shape. You aren't constrained to expected conditions.\n",
        "\n",
        "* The clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.\n",
        "\n",
        "**Distribution-based**\n",
        "\n",
        "* It is a clustering model in which we will fit the data on the probability that how it may belong to the same distribution.\n",
        "* There is a center-point established. As the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\n",
        "\n",
        "* This model works well on synthetic data and diversely sized clusters.\n",
        "\n",
        "* The method suffers from overfitting, unless constraints are put on the model complexity. A more complex model will usually be able to explain the data better, which makes choosing the appropriate model complexity inherently difficult."
      ],
      "metadata": {
        "id": "-Z-R0AIXJsvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set-up"
      ],
      "metadata": {
        "id": "in_Ft_XqJsvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 1 Data sets\n",
        "\n",
        "I decided to use 7 data sets. Six for exercise visualisations and one for solving real-data problem.\n",
        "\n",
        "Exercises sets:\n",
        "https://www.kaggle.com/datasets/joonasyoon/clustering-exercises\n",
        "\n",
        "Real-data set:\n",
        "https://www.kaggle.com/datasets/shrutipandit707/onlineretaildata"
      ],
      "metadata": {
        "id": "FReIWANfJsvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Import Libraries"
      ],
      "metadata": {
        "id": "GHsEfx91Jsvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:26.987396Z",
          "iopub.execute_input": "2022-11-22T14:56:26.988646Z",
          "iopub.status.idle": "2022-11-22T14:56:27.718484Z",
          "shell.execute_reply.started": "2022-11-22T14:56:26.988604Z",
          "shell.execute_reply": "2022-11-22T14:56:27.717334Z"
        },
        "trusted": true,
        "id": "LR9VOPprJsvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Import Data"
      ],
      "metadata": {
        "id": "3Qcz4qOmJsv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_df = pd.read_csv(\"../input/clustering-exercises/blob.csv\")\n",
        "dart_df = pd.read_csv(\"../input/clustering-exercises/dart.csv\")\n",
        "outliers_df = pd.read_csv(\"../input/clustering-exercises/outliers.csv\")\n",
        "spiral2_df = pd.read_csv(\"../input/clustering-exercises/spiral2.csv\")\n",
        "basic2_df = pd.read_csv(\"../input/clustering-exercises/basic2.csv\")\n",
        "boxes3_df = pd.read_csv(\"../input/clustering-exercises/boxes3.csv\")\n",
        "\n",
        "raw_df = pd.read_csv(\"../input/onlineretaildata/Online_Retail.csv\", encoding= 'unicode_escape')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:27.720218Z",
          "iopub.execute_input": "2022-11-22T14:56:27.720533Z",
          "iopub.status.idle": "2022-11-22T14:56:29.41696Z",
          "shell.execute_reply.started": "2022-11-22T14:56:27.720504Z",
          "shell.execute_reply": "2022-11-22T14:56:29.415875Z"
        },
        "trusted": true,
        "id": "I0EpT-vLJsv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Some Visualisations"
      ],
      "metadata": {
        "id": "R0P2fW-mJsv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3,figsize=(13,13))\n",
        "fig.suptitle('EXERCISE DATA SETS\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Blob\");\n",
        "\n",
        "axes[0,1].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"Dart\");\n",
        "\n",
        "axes[0,2].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,2].set_title(\"Basic\");\n",
        "\n",
        "axes[1,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Outliers\");\n",
        "\n",
        "axes[1,1].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"Spiral\");\n",
        "\n",
        "axes[1,2].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,2].set_title(\"Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:29.418653Z",
          "iopub.execute_input": "2022-11-22T14:56:29.419643Z",
          "iopub.status.idle": "2022-11-22T14:56:31.773027Z",
          "shell.execute_reply.started": "2022-11-22T14:56:29.419596Z",
          "shell.execute_reply": "2022-11-22T14:56:31.772102Z"
        },
        "trusted": true,
        "id": "X1SXo6BWJsv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Feature engineering"
      ],
      "metadata": {
        "id": "cCEAU3KHJsv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real-life data need some preparation before clustering."
      ],
      "metadata": {
        "id": "543AqrTJJsv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.describe(include='all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:31.77501Z",
          "iopub.execute_input": "2022-11-22T14:56:31.775375Z",
          "iopub.status.idle": "2022-11-22T14:56:32.202995Z",
          "shell.execute_reply.started": "2022-11-22T14:56:31.775341Z",
          "shell.execute_reply": "2022-11-22T14:56:32.202074Z"
        },
        "trusted": true,
        "id": "ZFzVC9u4Jsv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping columns\n",
        "raw_df.drop(['StockCode', 'InvoiceDate','Description','Country'],axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.204358Z",
          "iopub.execute_input": "2022-11-22T14:56:32.204725Z",
          "iopub.status.idle": "2022-11-22T14:56:32.238399Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.204676Z",
          "shell.execute_reply": "2022-11-22T14:56:32.237515Z"
        },
        "trusted": true,
        "id": "G5KDq9FVJsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_df[\"Quantity\"].min())\n",
        "print(raw_df[\"UnitPrice\"].min())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.239978Z",
          "iopub.execute_input": "2022-11-22T14:56:32.240587Z",
          "iopub.status.idle": "2022-11-22T14:56:32.250933Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.240553Z",
          "shell.execute_reply": "2022-11-22T14:56:32.24923Z"
        },
        "trusted": true,
        "id": "hOaMBDddJsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative values probably mean that there were returns. This is important factor for e-commerce, but for this study let's keep only transactions without return (our goal is to introduce and compare different clustering methods)."
      ],
      "metadata": {
        "id": "P1V4WVKEJsv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = raw_df.loc[raw_df[\"Quantity\"] >0 ]\n",
        "df = df.loc[df[\"UnitPrice\"] >0 ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.252475Z",
          "iopub.execute_input": "2022-11-22T14:56:32.252866Z",
          "iopub.status.idle": "2022-11-22T14:56:32.329583Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.25283Z",
          "shell.execute_reply": "2022-11-22T14:56:32.328025Z"
        },
        "trusted": true,
        "id": "ktK9OkXxJsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Quantity\"].min())\n",
        "print(df[\"UnitPrice\"].min())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.331116Z",
          "iopub.execute_input": "2022-11-22T14:56:32.331486Z",
          "iopub.status.idle": "2022-11-22T14:56:32.340855Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.331454Z",
          "shell.execute_reply": "2022-11-22T14:56:32.33951Z"
        },
        "trusted": true,
        "id": "uE5mhpKcJsv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new column of Total amount\n",
        "df[\"Total\"]=df[\"Quantity\"]*df[\"UnitPrice\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.342812Z",
          "iopub.execute_input": "2022-11-22T14:56:32.343529Z",
          "iopub.status.idle": "2022-11-22T14:56:32.35839Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.343476Z",
          "shell.execute_reply": "2022-11-22T14:56:32.357037Z"
        },
        "trusted": true,
        "id": "kI4gKrz5Jsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping 'Quantity' and 'UnitPrice'\n",
        "df.drop(['Quantity', 'UnitPrice'],axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.362588Z",
          "iopub.execute_input": "2022-11-22T14:56:32.363069Z",
          "iopub.status.idle": "2022-11-22T14:56:32.393995Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.363035Z",
          "shell.execute_reply": "2022-11-22T14:56:32.392691Z"
        },
        "trusted": true,
        "id": "vISn25FIJsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking data for the missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.395357Z",
          "iopub.execute_input": "2022-11-22T14:56:32.395684Z",
          "iopub.status.idle": "2022-11-22T14:56:32.437271Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.395656Z",
          "shell.execute_reply": "2022-11-22T14:56:32.435985Z"
        },
        "trusted": true,
        "id": "JwXyuDwZJsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the missing value in customerId .\n",
        "df.dropna(axis = 0, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.438615Z",
          "iopub.execute_input": "2022-11-22T14:56:32.439201Z",
          "iopub.status.idle": "2022-11-22T14:56:32.489632Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.439168Z",
          "shell.execute_reply": "2022-11-22T14:56:32.488737Z"
        },
        "trusted": true,
        "id": "oniGJGXtJsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating new features:**\n",
        "\n",
        "1. Frequency: total number of transactions\n",
        "\n",
        "2. Amount: total amount of transactions"
      ],
      "metadata": {
        "id": "V9tHViK0Jsv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Amount\n",
        "Amount = df.groupby('CustomerID')['Total'].sum()\n",
        "Amount = Amount.reset_index()\n",
        "Amount.columns=['CustomerID','Amount']\n",
        "\n",
        "#frequency\n",
        "Frequency=df.groupby('CustomerID')['InvoiceNo'].count()\n",
        "Frequency=Frequency.reset_index()\n",
        "Frequency.columns=['CustomerID','Frequency']\n",
        "\n",
        "# merge both df\n",
        "df1 = pd.merge(Amount, Frequency, on='CustomerID', how='inner')\n",
        "df1.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.490943Z",
          "iopub.execute_input": "2022-11-22T14:56:32.492222Z",
          "iopub.status.idle": "2022-11-22T14:56:32.570978Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.492179Z",
          "shell.execute_reply": "2022-11-22T14:56:32.569944Z"
        },
        "trusted": true,
        "id": "gGdEdIHzJsv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping 'CustomerID'\n",
        "df1.drop(['CustomerID'],axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.571911Z",
          "iopub.execute_input": "2022-11-22T14:56:32.573161Z",
          "iopub.status.idle": "2022-11-22T14:56:32.578636Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.573129Z",
          "shell.execute_reply": "2022-11-22T14:56:32.577878Z"
        },
        "trusted": true,
        "id": "MhedOmi1Jsv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Outlier detection"
      ],
      "metadata": {
        "id": "QyCzv42mJsv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.describe(include='all')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.580001Z",
          "iopub.execute_input": "2022-11-22T14:56:32.580377Z",
          "iopub.status.idle": "2022-11-22T14:56:32.6029Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.580346Z",
          "shell.execute_reply": "2022-11-22T14:56:32.601761Z"
        },
        "trusted": true,
        "id": "R-N9ssulJsv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(6,6))\n",
        "fig.suptitle('Outliers\\n', size = 25)\n",
        "\n",
        "sns.boxplot(ax=axes[0], data=df1['Amount'], palette='Spectral').set_title(\"Amount\")\n",
        "sns.boxplot(ax=axes[1], data=df1['Frequency'], palette='Spectral').set_title(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.604148Z",
          "iopub.execute_input": "2022-11-22T14:56:32.605132Z",
          "iopub.status.idle": "2022-11-22T14:56:32.952999Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.605099Z",
          "shell.execute_reply": "2022-11-22T14:56:32.951611Z"
        },
        "trusted": true,
        "id": "JkpXNrO-Jsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like we have significant problem with outliers."
      ],
      "metadata": {
        "id": "QHUKkemMJsv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Outlier detection model  selection\n",
        "* distribution is not normal\n",
        "* distribution is highly skewed\n",
        "* we have huge outliers\n",
        "\n",
        "Isolation Forest does not assume normal distribution and is able to detect outliers at a multi-dimensional level. Isolation Forest is also computationally efficient. The algorithm is based on the principle that anomalies are observations that are few and different, this should make them easier to identify. That's why I choose Isolation Forest.\n",
        "\n",
        "For oulier detection methods look here:\n",
        "https://www.kaggle.com/code/marcinrutecki/outlier-detection-methods"
      ],
      "metadata": {
        "id": "yxHTudm9Jsv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "df2 = df1.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:32.954391Z",
          "iopub.execute_input": "2022-11-22T14:56:32.954757Z",
          "iopub.status.idle": "2022-11-22T14:56:33.29655Z",
          "shell.execute_reply.started": "2022-11-22T14:56:32.954692Z",
          "shell.execute_reply": "2022-11-22T14:56:33.295662Z"
        },
        "trusted": true,
        "id": "myqnqgOFJsv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model building\n",
        "model=IsolationForest(n_estimators=150, max_samples='auto', contamination=float(0.1), max_features=1.0)\n",
        "model.fit(df2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:33.298113Z",
          "iopub.execute_input": "2022-11-22T14:56:33.298852Z",
          "iopub.status.idle": "2022-11-22T14:56:33.780429Z",
          "shell.execute_reply.started": "2022-11-22T14:56:33.298815Z",
          "shell.execute_reply": "2022-11-22T14:56:33.779196Z"
        },
        "trusted": true,
        "id": "CHXsnOjhJsv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 'scores' and 'anomaly' colums to df\n",
        "scores=model.decision_function(df2)\n",
        "anomaly=model.predict(df2)\n",
        "\n",
        "df2['scores']=scores\n",
        "df2['anomaly']=anomaly\n",
        "\n",
        "anomaly = df2.loc[df2['anomaly']==-1]\n",
        "anomaly_index = list(anomaly.index)\n",
        "print('Total number of outliers is:', len(anomaly))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:33.781766Z",
          "iopub.execute_input": "2022-11-22T14:56:33.78218Z",
          "iopub.status.idle": "2022-11-22T14:56:34.236108Z",
          "shell.execute_reply.started": "2022-11-22T14:56:33.78215Z",
          "shell.execute_reply": "2022-11-22T14:56:34.234919Z"
        },
        "trusted": true,
        "id": "ZP3QSCEjJsv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping outliers\n",
        "df2 = df2.drop(anomaly_index, axis = 0).reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:34.237372Z",
          "iopub.execute_input": "2022-11-22T14:56:34.237801Z",
          "iopub.status.idle": "2022-11-22T14:56:34.244198Z",
          "shell.execute_reply.started": "2022-11-22T14:56:34.23777Z",
          "shell.execute_reply": "2022-11-22T14:56:34.242936Z"
        },
        "trusted": true,
        "id": "OSaNeiY3Jsv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(6,6))\n",
        "fig.suptitle('Outliers\\n', size = 25)\n",
        "\n",
        "sns.boxplot(ax=axes[0], data=df2['Amount'], palette='Spectral').set_title(\"Amount\")\n",
        "sns.boxplot(ax=axes[1], data=df2['Frequency'], palette='Spectral').set_title(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:34.246089Z",
          "iopub.execute_input": "2022-11-22T14:56:34.246435Z",
          "iopub.status.idle": "2022-11-22T14:56:34.584743Z",
          "shell.execute_reply.started": "2022-11-22T14:56:34.246404Z",
          "shell.execute_reply": "2022-11-22T14:56:34.583592Z"
        },
        "trusted": true,
        "id": "duR6jYj4JswB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Isolation Forest algorithm did well!"
      ],
      "metadata": {
        "id": "1V3vTitXJswB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping columns that we don't need any more\n",
        "df2.drop(['scores', 'anomaly'], axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:34.586268Z",
          "iopub.execute_input": "2022-11-22T14:56:34.586598Z",
          "iopub.status.idle": "2022-11-22T14:56:34.592367Z",
          "shell.execute_reply.started": "2022-11-22T14:56:34.586567Z",
          "shell.execute_reply": "2022-11-22T14:56:34.591258Z"
        },
        "trusted": true,
        "id": "s_6R3zIaJswC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Scalling data"
      ],
      "metadata": {
        "id": "KOCZwlNKJswC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df3=scaler.fit_transform(df2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:34.593777Z",
          "iopub.execute_input": "2022-11-22T14:56:34.594158Z",
          "iopub.status.idle": "2022-11-22T14:56:34.607908Z",
          "shell.execute_reply.started": "2022-11-22T14:56:34.594128Z",
          "shell.execute_reply": "2022-11-22T14:56:34.606999Z"
        },
        "trusted": true,
        "id": "R_rLT6sIJswC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Determining The Optimal Number Of Clusters"
      ],
      "metadata": {
        "id": "txB6715-JswC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting optimal number of clusters is key to applying clustering algorithm to the dataset, such as k-means clustering, which requires the user to specify the number of clusters k to be generated. This is a somewhat arbitrary procedure, one of the weakest aspects of performing cluster analysis.\n",
        "\n",
        "The major difference between **elbow** and **silhouette method** is that elbow only calculates the euclidean distance whereas silhouette takes into account variables such as variance, skewness, high-low differences, etc.\n",
        "\n",
        "Both the Elbow method / SSE Plot and the Silhouette method can be used interchangeably based on the details presented by the plots."
      ],
      "metadata": {
        "id": "q3oVSefDJswC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e4-6KsJmJswC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Elbow method\n",
        "\n",
        "Elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use.\n",
        "\n",
        "The elbow method is a graphical representation of finding the optimal 'K' in a K-means clustering. It works by finding WCSS (Within-Cluster Sum of Square) i.e. the sum of the square distance between points in a cluster and the cluster centroid.\n",
        "\n",
        "The optimal number of clusters can be defined as follow:\n",
        "\n",
        "* Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n",
        "* For each k, calculate the total within-cluster sum of square (WSS).\n",
        "* Plot the curve of wss according to the number of clusters k.\n",
        "* The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters."
      ],
      "metadata": {
        "id": "w8cswToSJswC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
        "    kmeans.fit(df3)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:34.609513Z",
          "iopub.execute_input": "2022-11-22T14:56:34.610605Z",
          "iopub.status.idle": "2022-11-22T14:56:39.794222Z",
          "shell.execute_reply.started": "2022-11-22T14:56:34.610563Z",
          "shell.execute_reply": "2022-11-22T14:56:39.792859Z"
        },
        "trusted": true,
        "id": "gAZrY_-pJswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "model = KMeans()\n",
        "# k is range of number of clusters.\n",
        "visualizer = KElbowVisualizer(model, k=(2,30), timings= True)\n",
        "visualizer.fit(df3)        # Fit data to visualizer\n",
        "visualizer.show()        # Finalize and render figure"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:39.795753Z",
          "iopub.execute_input": "2022-11-22T14:56:39.79609Z",
          "iopub.status.idle": "2022-11-22T14:56:48.8614Z",
          "shell.execute_reply.started": "2022-11-22T14:56:39.796054Z",
          "shell.execute_reply": "2022-11-22T14:56:48.860295Z"
        },
        "trusted": true,
        "id": "vUi7wPXBJswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Silhouette method\n",
        "\n",
        "Average silhouette approach measures the **quality of a clustering**. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k (Kaufman and Rousseeuw 1990).\n",
        "\n",
        "The algorithm can be computed as follow:\n",
        "\n",
        "* Compute clustering algorithm (e.g., k-means clustering) for different values of k.\n",
        "* For each k, calculate the average silhouette of observations (avg.sil).\n",
        "* Plot the curve of avg.sil according to the number of clusters k.\n",
        "* The location of the maximum is considered as the appropriate number of clusters.\n",
        "\n",
        "The silhouette score falls within the range [-1, 1].\n",
        "\n",
        "The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong/incorrect.\n"
      ],
      "metadata": {
        "id": "JAZPFAUdJswD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Instantiate the KMeans for 5 clusters\n",
        "km = KMeans(n_clusters=5, random_state=42)\n",
        "# Fit the KMeans model\n",
        "km.fit_predict(df3)\n",
        "# Calculate Silhoutte Score\n",
        "score = silhouette_score(df3, km.labels_, metric='euclidean')\n",
        "# Print the score\n",
        "print('Silhouetter Average Score: %.3f' % score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:48.862857Z",
          "iopub.execute_input": "2022-11-22T14:56:48.863196Z",
          "iopub.status.idle": "2022-11-22T14:56:49.364429Z",
          "shell.execute_reply.started": "2022-11-22T14:56:48.863148Z",
          "shell.execute_reply": "2022-11-22T14:56:49.363222Z"
        },
        "trusted": true,
        "id": "KbENqmInJswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "# Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier.\n",
        "# You can find the code to simply create Silhouette visualisation for K-Means clusters with n_cluster as 2, 3, 4, 5, 6, 7 below.\n",
        "\n",
        "fig, ax = plt.subplots(3, 2, figsize=(13,8))\n",
        "fig.suptitle('Silhouette Analysis for 2-7 Clusters', size = 18)\n",
        "plt.tight_layout()\n",
        "\n",
        "for i in [2, 3, 4, 5, 6, 7]:\n",
        "    '''\n",
        "    Create KMeans instance for different number of clusters\n",
        "    '''\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
        "    q, mod = divmod(i, 2)\n",
        "    '''\n",
        "    Create SilhouetteVisualizer instance with KMeans instance\n",
        "    Fit the visualizer\n",
        "    '''\n",
        "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
        "    visualizer.fit(df3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:49.366321Z",
          "iopub.execute_input": "2022-11-22T14:56:49.36704Z",
          "iopub.status.idle": "2022-11-22T14:56:55.325769Z",
          "shell.execute_reply.started": "2022-11-22T14:56:49.366994Z",
          "shell.execute_reply": "2022-11-22T14:56:55.324634Z"
        },
        "trusted": true,
        "id": "TvJPJGyCJswD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice here is not so obvious due to the following reasons:\n",
        "\n",
        "1. Presence of clusters with below average silhouette scores for 5-7 clusters.\n",
        "2. Wide fluctuations in the size of the silhouette plots.\n",
        "3. The fluctuation in size is not similar, but better for 6 and 7 clusters.\n",
        "4. The thickness of the silhouette plot representing each cluster also is a deciding point. Unfortunately we have one cluster which is significantly thicker than the others."
      ],
      "metadata": {
        "id": "-JLfm7M5JswE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Dendrogram\n",
        "\n",
        "This technique is specific to the agglomerative hierarchical method of clustering. The method starts by considering each point as a separate cluster and starts joining points to clusters in a hierarchical fashion based on their distances. To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows the sequences of merges or splits of clusters."
      ],
      "metadata": {
        "id": "V5rArl9XJswE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as sch\n",
        "from matplotlib import pyplot\n",
        "pyplot.figure(figsize=(12, 5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(df3, method = 'ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.ylabel('Euclidean distances')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:56:55.334491Z",
          "iopub.execute_input": "2022-11-22T14:56:55.334919Z",
          "iopub.status.idle": "2022-11-22T14:58:10.427345Z",
          "shell.execute_reply.started": "2022-11-22T14:56:55.334881Z",
          "shell.execute_reply": "2022-11-22T14:58:10.426163Z"
        },
        "trusted": true,
        "id": "9xvpwtECJswE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. K-Means\n",
        "\n",
        "K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n",
        "It's a centroid-based algorithm and the simplest unsupervised learning algorithm.\n",
        "The algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n",
        "\n",
        "**K-means++** (default init parameter for K-Means in sklearn) is the algorithm which is used to overcome the drawback posed by the k-means algorithm. The goal is to spread out the initial centroid by assigning the first centroid randomly then selecting the rest of the centroids based on the maximum squared distance. The idea is to push the centroids as far as possible from one another.\n",
        "\n",
        "Although the initialization in K-means++ is computationally more expensive than the standard K-means algorithm, the run-time for convergence to optimum is drastically reduced for K-means++. This is because the centroids that are initially chosen are likely to lie in different clusters already."
      ],
      "metadata": {
        "id": "op6Ts9aBJswE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Kmeans.png](attachment:f09a4346-57f3-4330-9abb-668f42800ba7.png)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:10.428952Z",
          "iopub.execute_input": "2022-11-22T14:58:10.429613Z",
          "iopub.status.idle": "2022-11-22T14:58:10.438368Z",
          "shell.execute_reply.started": "2022-11-22T14:58:10.42957Z",
          "shell.execute_reply": "2022-11-22T14:58:10.437217Z"
        },
        "id": "RiIayGbOJswF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Advantages and disadvantages of K-Means"
      ],
      "metadata": {
        "id": "SYgBlB_ZJswF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of K-Means clustering:**\n",
        "\n",
        "1. **Simple** - It is easy to implement\n",
        "\n",
        "2. **High Performance** - K-Means clustering technique is fast and efficient in terms of its computational cost\n",
        "\n",
        "4. **Easy to interpret** - K-Means returns clusters which can be easily interpreted and visualized\n",
        "\n",
        "5. **Suitable for large data sets** while clustering algorithms are known to be relatively slow, the k-means algorithm is comparatively fast, that's why it's effective for large data sets.\n",
        "\n",
        "\n",
        "**Disadvantages of K-Means clustering:**\n",
        "\n",
        "1. **Assumes spherical density** - this means that k-means clustering does not perform as well in situations where clusters naturally have irregular shapes. This is a relatively strict assumption.\n",
        "\n",
        "2. **Sensitive to scale** - if one of the variables is on a much larger scale than the others that variable will have an outsized effect on the distance calculated. This means that we generally need to re-scale data before using k-means clustering.\n",
        "\n",
        "3. **Difficult to incorporate categorical variables** - K-means is intended to be used when all of your features are numeric. There are ways you can adapt your data to be suitable if you have some categorical features, but in general the majority of your features should be numeric.\n",
        "\n",
        "4. **Sensitive to outliers** - K-Means has no no outlier detection method. Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored.\n",
        "\n",
        "5. **Initialization sensitivity** - K-means clustering is sensitive to the starting conditions that are used to initialize the algorithm such as the choice of seed or the order of the data points. If we get unlucky with a choice of starting points, the clusters produced can be arbitrarily bad. This one gets fixed with a variant known as k-means++ (default init for sklearn)\n",
        "\n",
        "6. **Have to choose the number of clusters** - K-means clustering requires to specify the number of clusters that will be created ahead of time. Picking the right value of 'k' is a challenging model selection problem.\n",
        "\n",
        "7. **Struggles with high dimensional data** - The algorithm depends on Euclidean distance, which is pretty bad in high dimensions. If we have many potential features, we should consider applying feature selection or dimensionality reduction algorithms to the data before creating clusters.\n",
        "\n",
        "8. **The algorithm is randomized** - which means that we can run it on the same data set multiple times and get different answers. This is a big issue for a lot of applications.\n",
        "\n",
        "9. **k-means can only separate clusters that are more or less linearly separable** - If your clusters are based on distance to the origin, k-means wonâ€™t be able to identify them. We can fix that by switching to polar coordinates."
      ],
      "metadata": {
        "id": "fWl_V494JswG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The ways to avoid the problem of initialization sensitivity in k-means algorithm:**\n",
        "\n",
        "1. **Repeat k-means** - the algorithm is executed repeatedly. The centroids are initialized and the clusters are formed to result in smallest intra-cluster distance and larger inter-cluster distance.\n",
        "\n",
        "2. **K-Means++** - a smart centroid initialization technique. Only one centroid is initialized randomly, and other centroids are chosen such that they are very far from the initial centroid/s. This results in faster convergence and lower possibility of the centroid being poorly initialized. Default init for sklearn.\n",
        "\n",
        "K means++ provides comparatively better results."
      ],
      "metadata": {
        "id": "ke6rmslaJswG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Variations of K-Means"
      ],
      "metadata": {
        "id": "DpCCpm7rJswG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some of the variations of the k-means algorithm:**\n",
        "\n",
        "1. **k-medians clustering** - uses the median in each dimension (instead of the mean).\n",
        "\n",
        "2. **k-medoids (Partitioning Around Medoids)** - uses medoid instead of mean, and minimizes the sum of distances for arbitrary distance functions.\n",
        "\n",
        "3. **Fuzzy C-Means clustering** - a soft version of k-means, where each data point has a fuzzy degree of belonging to each cluster.\n",
        "\n",
        "4. **k-means++** - standard k-means algorithm with a smarter initialization of the centroids (default init for sklearn)."
      ],
      "metadata": {
        "id": "w5vRhB83JswG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Training the K-Means model on the datasets"
      ],
      "metadata": {
        "id": "ryr58OGUJswG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying data sets\n",
        "df_kmeans = df3.copy()\n",
        "df_blob_kmeans = blob_df.copy()\n",
        "df_dart_kmeans = dart_df.copy()\n",
        "df_basic_kmeans = basic2_df.copy()\n",
        "df_outliers_kmeans = outliers_df.copy()\n",
        "df_spiral2_kmeans = spiral2_df.copy()\n",
        "df_boxes3_kmeans = boxes3_df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:10.439794Z",
          "iopub.execute_input": "2022-11-22T14:58:10.440131Z",
          "iopub.status.idle": "2022-11-22T14:58:10.454978Z",
          "shell.execute_reply.started": "2022-11-22T14:58:10.440101Z",
          "shell.execute_reply": "2022-11-22T14:58:10.453277Z"
        },
        "trusted": true,
        "id": "VaElXwQMJswH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_blob_kmeans.drop(['color'], axis = 1, inplace =True)\n",
        "df_dart_kmeans.drop(['color'], axis = 1, inplace =True)\n",
        "df_basic_kmeans.drop(['color'], axis = 1, inplace =True)\n",
        "df_outliers_kmeans.drop(['color'], axis = 1, inplace =True)\n",
        "df_spiral2_kmeans.drop(['color'], axis = 1, inplace =True)\n",
        "df_boxes3_kmeans.drop(['color'], axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:10.456622Z",
          "iopub.execute_input": "2022-11-22T14:58:10.457059Z",
          "iopub.status.idle": "2022-11-22T14:58:10.469388Z",
          "shell.execute_reply.started": "2022-11-22T14:58:10.457025Z",
          "shell.execute_reply": "2022-11-22T14:58:10.468168Z"
        },
        "trusted": true,
        "id": "TTaFgWKwJswH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "kmeans = KMeans(n_clusters = 6, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(df_kmeans)\n",
        "\n",
        "kmeans_blob = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)\n",
        "kmeans_dart = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
        "kmeans_basic = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
        "kmeans_outliers = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)\n",
        "kmeans_spiral2 = KMeans(n_clusters = 2, init = 'k-means++', random_state = 42)\n",
        "kmeans_boxes3 = KMeans(n_clusters = 12, init = 'k-means++', random_state = 42)\n",
        "\n",
        "y_kmeans_blob = kmeans_blob.fit_predict(df_blob_kmeans)\n",
        "y_kmeans_dart = kmeans_dart.fit_predict(df_dart_kmeans)\n",
        "y_kmeans_basic = kmeans_basic.fit_predict(df_basic_kmeans)\n",
        "y_kmeans_outliers = kmeans_outliers.fit_predict(df_outliers_kmeans)\n",
        "y_kmeans_spiral2 = kmeans_spiral2.fit_predict(df_spiral2_kmeans)\n",
        "y_kmeans_boxes3 = kmeans_boxes3.fit_predict(df_boxes3_kmeans)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:10.470681Z",
          "iopub.execute_input": "2022-11-22T14:58:10.471825Z",
          "iopub.status.idle": "2022-11-22T14:58:14.838236Z",
          "shell.execute_reply.started": "2022-11-22T14:58:10.471779Z",
          "shell.execute_reply": "2022-11-22T14:58:14.837313Z"
        },
        "trusted": true,
        "id": "WaCLM9D_JswH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'Cluster' columns in data sets\n",
        "df_blob_kmeans['Cluster'] = y_kmeans_blob\n",
        "df_dart_kmeans['Cluster'] = y_kmeans_dart\n",
        "df_basic_kmeans['Cluster'] = y_kmeans_basic\n",
        "df_outliers_kmeans['Cluster'] = y_kmeans_outliers\n",
        "df_spiral2_kmeans['Cluster'] = y_kmeans_spiral2\n",
        "df_boxes3_kmeans['Cluster'] = y_kmeans_boxes3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:14.839695Z",
          "iopub.execute_input": "2022-11-22T14:58:14.840919Z",
          "iopub.status.idle": "2022-11-22T14:58:14.85059Z",
          "shell.execute_reply.started": "2022-11-22T14:58:14.840875Z",
          "shell.execute_reply": "2022-11-22T14:58:14.849751Z"
        },
        "trusted": true,
        "id": "PIKO8HPUJswH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Comparing results"
      ],
      "metadata": {
        "id": "bgaeGTYYJswI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=6, ncols=2,figsize=(10,30))\n",
        "fig.suptitle('ANSWER vs K-Means\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Answer Blob\");\n",
        "axes[0,1].scatter(df_blob_kmeans['x'], df_blob_kmeans['y'], c=df_blob_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"K-Means Blob\");\n",
        "\n",
        "axes[1,0].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Answer Dart\");\n",
        "axes[1,1].scatter(df_dart_kmeans['x'], df_dart_kmeans['y'], c=df_dart_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"K-Means Dart\");\n",
        "\n",
        "axes[2,0].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[2,0].set_title(\"Answer Basic\");\n",
        "axes[2,1].scatter(df_basic_kmeans['x'], df_basic_kmeans['y'], c=df_basic_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,1].set_title(\"K-Means Basic\");\n",
        "\n",
        "axes[3,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[3,0].set_title(\"Answer Outliers\");\n",
        "axes[3,1].scatter(df_outliers_kmeans['x'], df_outliers_kmeans['y'], c=df_outliers_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,1].set_title(\"K-Means Outliers\");\n",
        "\n",
        "axes[4,0].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[4,0].set_title(\"Answer Spiral\");\n",
        "axes[4,1].scatter(df_spiral2_kmeans['x'], df_spiral2_kmeans['y'], c=df_spiral2_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,1].set_title(\"K-Means Spiral\");\n",
        "\n",
        "axes[5,0].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[5,0].set_title(\"Answer Boxes\");\n",
        "axes[5,1].scatter(df_boxes3_kmeans['x'], df_boxes3_kmeans['y'], c=df_boxes3_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,1].set_title(\"K-Means Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:14.852472Z",
          "iopub.execute_input": "2022-11-22T14:58:14.853646Z",
          "iopub.status.idle": "2022-11-22T14:58:19.903936Z",
          "shell.execute_reply.started": "2022-11-22T14:58:14.853601Z",
          "shell.execute_reply": "2022-11-22T14:58:19.902859Z"
        },
        "trusted": true,
        "id": "hOrP0xBkJswI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 K-Means on online retail data"
      ],
      "metadata": {
        "id": "qh02qFIpJswI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
        "df_kmeans = df2.copy()\n",
        "# Checking number of items in clusters and creating 'Cluster' column\n",
        "df_kmeans['Cluster'] = y_kmeans\n",
        "df_kmeans['Cluster'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:19.905636Z",
          "iopub.execute_input": "2022-11-22T14:58:19.906098Z",
          "iopub.status.idle": "2022-11-22T14:58:19.918662Z",
          "shell.execute_reply.started": "2022-11-22T14:58:19.90606Z",
          "shell.execute_reply": "2022-11-22T14:58:19.91743Z"
        },
        "trusted": true,
        "id": "kpH4_huQJswJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(data=df_kmeans, x='Amount', y='Frequency', hue = 'Cluster', s=15, palette=\"Set3\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:19.920427Z",
          "iopub.execute_input": "2022-11-22T14:58:19.921118Z",
          "iopub.status.idle": "2022-11-22T14:58:20.540171Z",
          "shell.execute_reply.started": "2022-11-22T14:58:19.921079Z",
          "shell.execute_reply": "2022-11-22T14:58:20.539347Z"
        },
        "trusted": true,
        "id": "-VAY9FUdJswJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Hierarchical clustering\n",
        "Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis that seeks to build a hierarchy of clusters. It's used to group objects in clusters based on how similar they are to each other. In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n",
        "\n",
        "Hierarchical clustering is particularly useful in situations where you have a few observations you are particularly interested in and you want to be able to identify observations that are similar to those observations.\n",
        "\n",
        "**Hierarchical clustering can be**:\n",
        "\n",
        "**Agglomerative** â€“ it starts with an individual element and then groups them into single clusters.\n",
        "\n",
        "**Divisive** â€“ it starts with a complete dataset and divides it into partitions.\n",
        "\n",
        "Agglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes."
      ],
      "metadata": {
        "id": "AE_lO0A-JswJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![HC.png](attachment:da111b36-71eb-4fbf-b0db-f6bb673bd518.png)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:20.541457Z",
          "iopub.execute_input": "2022-11-22T14:58:20.542617Z",
          "iopub.status.idle": "2022-11-22T14:58:20.549689Z",
          "shell.execute_reply.started": "2022-11-22T14:58:20.542485Z",
          "shell.execute_reply": "2022-11-22T14:58:20.548632Z"
        },
        "id": "0-trlA63JswJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Advantages and Disadvantages of Hierarchical clustering"
      ],
      "metadata": {
        "id": "MdzMRqucJswJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Hierarchical clustering:**\n",
        "\n",
        "1. **Get the most similar observations to any given observations** - the algorithm provides detailed information about which observations are most similar to each other. This level of detail is not provided by many other algorithms.\n",
        "\n",
        "2. **Not so sensitive to initialization conditions** - it is not sensitive to initialization conditions such as seeds that are set or the order of the dataset. You should generally get very similar results, and in some cases the same exact result, if you re-run your analysis with different initialization conditions.\n",
        "\n",
        "3. **Can be adapted to incorporate categorical variables** - it can be adapted to support situations where you have a mixture of numeric and categorical variables relatively easily. In order to do this, you must ensure that you are using a distance metrics that is appropriate for mixed data types such as Growerâ€™s distance.\n",
        "\n",
        "4. **Less sensitive to outliers** - the presence of a few outliers is not likely to affect the way the algorithm performs on the other data points.\n",
        "\n",
        "5. **Less stringent assumptions about cluster shape** - algorithms do not make as stringent assumptions about the shape of your clusters. Depending on the distance metric you use, some cluster shapes may be detected more easily than others, but there is more flexibility.\n",
        "\n",
        "\n",
        "**Disadvantages of Hierarchical clustering:**\n",
        "\n",
        "\n",
        "1. **Relatively slow** - it is a mathematically very heavy algorithm. Hierarchical clustering generally requires to compute the pairwise distance between all of the observations in a dataset, so the number of computations required grows rapidly as the size of your dataset increases.\n",
        "\n",
        "2. **Have to specify the number of clusters** - but the number of clusters can be changed after the main part of the algorithm has run, so we can experiment with using different numbers of clusters without having to run the algorithm from scratch.\n",
        "\n",
        "3. **Sensitive to scale** - we may need to rescale data before running clustering. The exact level of sensitivity will vary depending on what distance metric we are using to calculate the distance between points.\n",
        "\n",
        "4. **Heavily driven by heuristics** - this leads to a lot of manual intervention in the process and consequently, application/domain-specific knowledge is required to analyze whether the result makes any sense or not.\n",
        "\n",
        "5. **Possibly difficult to visualize** - If the number of data samples increases, then visually analyzing dendrogram and making decisions becomes impossible."
      ],
      "metadata": {
        "id": "rnRGCscKJswK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Variations of hierarchical clustering"
      ],
      "metadata": {
        "id": "e889dEzlJswM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIRCH** is an extension of hierarchical clustering that runs faster on large datasets. It has lower memory requirements than standard hierarchical clustering."
      ],
      "metadata": {
        "id": "_fcK8gHuJswN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Training the hierarchical clustering model on the datasets"
      ],
      "metadata": {
        "id": "Wab1Gq15JswN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying data sets\n",
        "df_AgglomerativeC = df3.copy()\n",
        "df_blob_AgglomerativeC = blob_df.copy()\n",
        "df_dart_AgglomerativeC = dart_df.copy()\n",
        "df_basic2_AgglomerativeC = basic2_df.copy()\n",
        "df_outliers_AgglomerativeC = outliers_df.copy()\n",
        "df_spiral2_AgglomerativeC = spiral2_df.copy()\n",
        "df_boxes3_AgglomerativeC = boxes3_df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:20.551152Z",
          "iopub.execute_input": "2022-11-22T14:58:20.551729Z",
          "iopub.status.idle": "2022-11-22T14:58:20.56198Z",
          "shell.execute_reply.started": "2022-11-22T14:58:20.551679Z",
          "shell.execute_reply": "2022-11-22T14:58:20.560817Z"
        },
        "trusted": true,
        "id": "YGFENLxGJswN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_blob_AgglomerativeC.drop(['color'], axis = 1, inplace =True)\n",
        "df_dart_AgglomerativeC.drop(['color'], axis = 1, inplace =True)\n",
        "df_basic2_AgglomerativeC.drop(['color'], axis = 1, inplace =True)\n",
        "df_outliers_AgglomerativeC.drop(['color'], axis = 1, inplace =True)\n",
        "df_spiral2_AgglomerativeC.drop(['color'], axis = 1, inplace =True)\n",
        "df_boxes3_AgglomerativeC.drop(['color'], axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:20.565487Z",
          "iopub.execute_input": "2022-11-22T14:58:20.565923Z",
          "iopub.status.idle": "2022-11-22T14:58:20.579357Z",
          "shell.execute_reply.started": "2022-11-22T14:58:20.565871Z",
          "shell.execute_reply": "2022-11-22T14:58:20.578137Z"
        },
        "trusted": true,
        "id": "s8CUAe6QJswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Training model\n",
        "AgglomerativeC = AgglomerativeClustering(n_clusters=6, affinity = 'euclidean', linkage = 'ward')\n",
        "y_AgglomerativeC = AgglomerativeC.fit_predict(df_AgglomerativeC)\n",
        "\n",
        "AgglomerativeC_blob = AgglomerativeClustering(n_clusters = 4, affinity = 'euclidean', linkage = 'ward')\n",
        "AgglomerativeC_dart = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "AgglomerativeC_basic = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\n",
        "AgglomerativeC_outliers = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\n",
        "AgglomerativeC_spiral2 = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "AgglomerativeC_boxes3 = AgglomerativeClustering(n_clusters = 12, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "y_AgglomerativeC_blob = AgglomerativeC_blob.fit_predict(df_blob_AgglomerativeC)\n",
        "y_AgglomerativeC_dart = AgglomerativeC_dart.fit_predict(df_dart_AgglomerativeC)\n",
        "y_AgglomerativeC_basic = AgglomerativeC_basic.fit_predict(df_basic2_AgglomerativeC)\n",
        "y_AgglomerativeC_outliers = AgglomerativeC_outliers.fit_predict(df_outliers_AgglomerativeC)\n",
        "y_AgglomerativeC_spiral2 = AgglomerativeC_spiral2.fit_predict(df_spiral2_AgglomerativeC)\n",
        "y_AgglomerativeC_boxes3 = AgglomerativeC_boxes3.fit_predict(df_boxes3_AgglomerativeC)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:20.580747Z",
          "iopub.execute_input": "2022-11-22T14:58:20.581444Z",
          "iopub.status.idle": "2022-11-22T14:58:45.972552Z",
          "shell.execute_reply.started": "2022-11-22T14:58:20.581413Z",
          "shell.execute_reply": "2022-11-22T14:58:45.971187Z"
        },
        "trusted": true,
        "id": "KXDrN6WWJswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'Cluster' columns in data sets\n",
        "df_blob_AgglomerativeC['Cluster'] = y_AgglomerativeC_blob\n",
        "df_dart_AgglomerativeC['Cluster'] = y_AgglomerativeC_dart\n",
        "df_basic2_AgglomerativeC['Cluster'] = y_AgglomerativeC_basic\n",
        "df_outliers_AgglomerativeC['Cluster'] = y_AgglomerativeC_outliers\n",
        "df_spiral2_AgglomerativeC['Cluster'] = y_AgglomerativeC_spiral2\n",
        "df_boxes3_AgglomerativeC['Cluster'] = y_AgglomerativeC_boxes3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:45.974528Z",
          "iopub.execute_input": "2022-11-22T14:58:45.976621Z",
          "iopub.status.idle": "2022-11-22T14:58:45.984753Z",
          "shell.execute_reply.started": "2022-11-22T14:58:45.976582Z",
          "shell.execute_reply": "2022-11-22T14:58:45.983947Z"
        },
        "trusted": true,
        "id": "CEIibHYzJswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Comparing results"
      ],
      "metadata": {
        "id": "bNVunLqwJswO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=6, ncols=2,figsize=(10,30))\n",
        "fig.suptitle('ANSWER vs Hierarchical clustering\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Answer Blob\");\n",
        "axes[0,1].scatter(df_blob_AgglomerativeC['x'], df_blob_AgglomerativeC['y'], c=df_blob_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"Hierarchical clustering Blob\");\n",
        "\n",
        "axes[1,0].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Answer Dart\");\n",
        "axes[1,1].scatter(df_dart_AgglomerativeC['x'], df_dart_AgglomerativeC['y'], c=df_dart_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"Hierarchical clustering Dart\");\n",
        "\n",
        "axes[2,0].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[2,0].set_title(\"Answer Basic\");\n",
        "axes[2,1].scatter(df_basic2_AgglomerativeC['x'], df_basic2_AgglomerativeC['y'], c=df_basic2_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,1].set_title(\"Hierarchical clustering Basic\");\n",
        "\n",
        "axes[3,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[3,0].set_title(\"Answer Outliers\");\n",
        "axes[3,1].scatter(df_outliers_AgglomerativeC['x'], df_outliers_AgglomerativeC['y'], c=df_outliers_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,1].set_title(\"Hierarchical clustering Outliers\");\n",
        "\n",
        "axes[4,0].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[4,0].set_title(\"Answer Spiral\");\n",
        "axes[4,1].scatter(df_spiral2_AgglomerativeC['x'], df_spiral2_AgglomerativeC['y'], c=df_spiral2_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,1].set_title(\"Hierarchical clustering Spiral\");\n",
        "\n",
        "axes[5,0].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[5,0].set_title(\"Answer Boxes\");\n",
        "axes[5,1].scatter(df_boxes3_AgglomerativeC['x'], df_boxes3_AgglomerativeC['y'], c=df_boxes3_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,1].set_title(\"Hierarchical clustering Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:45.985825Z",
          "iopub.execute_input": "2022-11-22T14:58:45.986346Z",
          "iopub.status.idle": "2022-11-22T14:58:50.494147Z",
          "shell.execute_reply.started": "2022-11-22T14:58:45.986316Z",
          "shell.execute_reply": "2022-11-22T14:58:50.492933Z"
        },
        "trusted": true,
        "id": "gUrXAMvgJswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 Hierarchical clustering on online retail data"
      ],
      "metadata": {
        "id": "Hyy-AAgOJswP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
        "df_AgglomerativeC = df2.copy()\n",
        "# Checking number of items in clusters and creating 'Cluster' column\n",
        "df_AgglomerativeC['Cluster'] = y_AgglomerativeC\n",
        "df_AgglomerativeC['Cluster'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:50.495802Z",
          "iopub.execute_input": "2022-11-22T14:58:50.496578Z",
          "iopub.status.idle": "2022-11-22T14:58:50.50708Z",
          "shell.execute_reply.started": "2022-11-22T14:58:50.496542Z",
          "shell.execute_reply": "2022-11-22T14:58:50.505997Z"
        },
        "trusted": true,
        "id": "5iXiCIfDJswP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(data=df_AgglomerativeC, x='Amount', y='Frequency', hue = 'Cluster', s=15, palette=\"Set3\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:50.509104Z",
          "iopub.execute_input": "2022-11-22T14:58:50.50951Z",
          "iopub.status.idle": "2022-11-22T14:58:51.082206Z",
          "shell.execute_reply.started": "2022-11-22T14:58:50.509472Z",
          "shell.execute_reply": "2022-11-22T14:58:51.081006Z"
        },
        "trusted": true,
        "id": "UWsPYZj1JswQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. DBSCAN clustering algorithm\n",
        "DBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm.\n",
        "It is able to find irregular-shaped clusters. It separates regions by areas of low-density so it can also detect outliers really well. This algorithm is better than k-means when it comes to working with oddly shaped data."
      ],
      "metadata": {
        "id": "76jzKhXyJswQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![DBscan.png](attachment:ae50112d-c04f-4e20-a3c7-8a53d5b739d8.png)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.083636Z",
          "iopub.execute_input": "2022-11-22T14:58:51.084018Z",
          "iopub.status.idle": "2022-11-22T14:58:51.091667Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.083987Z",
          "shell.execute_reply": "2022-11-22T14:58:51.0905Z"
        },
        "id": "tBZ1PTBNJswQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Advantages and Disadvantages of DBSCAN"
      ],
      "metadata": {
        "id": "puaQAWOHJswQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of DBSCAN:**\n",
        "1. **Handles irregularly shaped and sized clusters** - DBSCAN is one of the algorithms that makes the fewest assumptions about the shape of clusters. That means that DBSCAN can be used to detect clusters that are oddly or irregularly shaped.\n",
        "\n",
        "2. **Robust to outliers** - the algorithm it is able to detect outliers and exclude them from the clusters entirely.\n",
        "\n",
        "3. **Does not require the number of clusters to be specified** - another advantage of DBSCAN is that it does not require the user to specify the number of clusters. DBSCAN can automatically detect the number of clusters that exist in the data. This is great for cases where you do not have much intuition on how many clusters there should be.\n",
        "\n",
        "4. **Less sensitive to initialization conditions** - DBSCAN is less sensitive to initialization conditions like the order of the observations in the dataset and the seed that is used than some other clustering algorithms.\n",
        "\n",
        "5. **Relatively fast** - DBSCAN is generally slower than K-Means clustering, but faster than hierarchical clustering and spectral clustering.\n",
        "\n",
        "\n",
        "**Disadvantages of DBSCAN:**\n",
        "1. **Difficult to incorporate categorical features** - you should use DBSCAN in cases where most of your features are numeric.\n",
        "\n",
        "2. **Requires a drop in density to detect cluster borders** - there must be a drop in the density of the data points between clusters in order for the algorithm to be able to detect the boundaries between clusters. If there are multiple clusters that are overlapping without a drop in data density between them, they may get grouped into a single cluster.\n",
        "\n",
        "3. **Struggles with clusters of varying density** - DBSCAN determines where clusters start and stop by looking at places where the density of data points drops below a certain threshold. It may be difficult to find a threshold that captures all of the points in the less dense cluster without excluding too many extraneous outliers in the more dense cluster.\n",
        "\n",
        "4. **Sensitive to scale** - DBSCAN is sensitive to the scale of variables. That means that we need to rescale variables if they are on very different scales.\n",
        "\n",
        "5. **Struggles with high dimensional data** - performance of DBSCAN tends to degrade in situations where there are many features. We should use dimensionality reduction or features selection techniques to reduce the number of features if we have a high-dimensional dataset.\n",
        "\n",
        "6. **Need to set-up parameters for Epsilon and MinPts**. This could be tricky.\n",
        "\n"
      ],
      "metadata": {
        "id": "0nBhpn7YJswQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Choosing the right initial parameters"
      ],
      "metadata": {
        "id": "g1CzGoAVJswT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN requires two parameters - choosing the right numbers is critical for this algorithm:\n",
        "\n",
        "1. **Epsilon (Îµ)** is the radius of the circle to be created around each data point to check the density.\n",
        "\n",
        "This technique calculates the average distance between each point and its k nearest neighbors, where k = the MinPts value you selected. The average k-distances are then plotted in ascending order on a k-distance graph. The value of **epsilon** can be decided from the K-distance graph. The point of maximum curvature (elbow) in this graph tells us about the value of epsilon (i.e. where the graph has the greatest slope).\n",
        "\n",
        "\n",
        "2. **minPoints (MinPts)** is the minimum number of data points required inside that circle for that data point to be classified as a Core point.\n",
        "\n",
        "There is no automatic way to determine the MinPts value for DBSCAN. It does not make sense to take minPoints as 1 because it will result in each point being a separate cluster. Therefore, it must be at least 3. Generally, it is twice the dimensions or dimensions +1. Domain knowledge also decides its value.\n",
        "\n",
        "* If the data set is noisier, choose a larger value of MinPts\n",
        "* For 2-dimensional data, use DBSCANâ€™s default value of MinPts = 4 (Ester et al., 1996).\n",
        "\n",
        "The value of minPoints should be at least one greater than the number of dimensions of the dataset:\n",
        "\n",
        "* MinPts >= Dimensions * 2\n",
        "\n",
        "OR\n",
        "\n",
        "* MinPts >= Dimensions + 1\n",
        "\n",
        "**IMPORTANT:**\n",
        "\n",
        "Sometimes it is difficult to find suitable parameters of Epsilon and MinPts for data set. These are some hints:\n",
        "\n",
        "1. Use a larger MinPts for large and noisy data sets.\n",
        "\n",
        "2. If you get too large clusters: decrease epsilon.\n",
        "\n",
        "3. If you get too much noise: increase epsilon).\n",
        "\n",
        "4. Good clustering requires iterations."
      ],
      "metadata": {
        "id": "51Ihlz6xJswU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![eps.png](attachment:cf50ef28-b2fa-4769-9496-2b72c3cc1731.png)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.094838Z",
          "iopub.execute_input": "2022-11-22T14:58:51.095161Z",
          "iopub.status.idle": "2022-11-22T14:58:51.1056Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.095133Z",
          "shell.execute_reply": "2022-11-22T14:58:51.104275Z"
        },
        "id": "PiNraJuoJswU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For plotting a K-distance Graph, we need a distance between a point and its nearest data point for all data points in the dataset. We obtain this using NearestNeighbors from sklearn.neighbors."
      ],
      "metadata": {
        "id": "x3klDX-hJswU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying data sets\n",
        "df_DBScan = df3.copy()\n",
        "df_blob_DBScan = blob_df.copy()\n",
        "df_dart_DBScan = dart_df.copy()\n",
        "df_basic2_DBScan = basic2_df.copy()\n",
        "df_outliers_DBScan = outliers_df.copy()\n",
        "df_spiral2_DBScan = spiral2_df.copy()\n",
        "df_boxes3_DBScan = boxes3_df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.107131Z",
          "iopub.execute_input": "2022-11-22T14:58:51.107521Z",
          "iopub.status.idle": "2022-11-22T14:58:51.115947Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.107491Z",
          "shell.execute_reply": "2022-11-22T14:58:51.115017Z"
        },
        "trusted": true,
        "id": "2JuTsBdgJswU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "nn = NearestNeighbors(n_neighbors=4)\n",
        "nbrs = nn.fit(df_dart_DBScan)\n",
        "distances, indices = nbrs.kneighbors(df_dart_DBScan)\n",
        "\n",
        "# Plotting K-distance Graph\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(distances)\n",
        "plt.title('K-distance Graph for \"Dart df\"',fontsize=20)\n",
        "plt.xlabel('Data Points sorted by distance',fontsize=14)\n",
        "plt.ylabel('Epsilon',fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.117533Z",
          "iopub.execute_input": "2022-11-22T14:58:51.118182Z",
          "iopub.status.idle": "2022-11-22T14:58:51.320016Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.118148Z",
          "shell.execute_reply": "2022-11-22T14:58:51.318483Z"
        },
        "trusted": true,
        "id": "6qjm_r-0JswU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 8 in this case.\n",
        "2. For MinPts I'll choose 4 (2 * dimensions)"
      ],
      "metadata": {
        "id": "HFBvpnktJswU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Variations of DBSCAN"
      ],
      "metadata": {
        "id": "MbsLOCfLJswU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized DBSCAN (GDBSCAN)** - the Îµ and minPts parameters are removed from the original algorithm and moved to the predicates.\n",
        "\n",
        "**OPTICS (Ordering Points To Identify the Clustering Structure)** - finds core sample of high density and expands clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.\n",
        "\n",
        "**HDBSCAN** - hierarchical version of DBSCAN which is also faster than OPTICS, from which a flat partition consisting of the most prominent clusters can be extracted from the hierarchy.\n",
        "\n",
        "DBSCAN is also used as part of subspace clustering algorithms like PreDeCon and SUBCLU."
      ],
      "metadata": {
        "id": "Qri0aE8FJswU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Training of DBSCAN clustering model on the datasets"
      ],
      "metadata": {
        "id": "rti40SmkJswV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_blob_DBScan.drop(['color'], axis = 1, inplace =True)\n",
        "df_dart_DBScan.drop(['color'], axis = 1, inplace =True)\n",
        "df_basic2_DBScan.drop(['color'], axis = 1, inplace =True)\n",
        "df_outliers_DBScan.drop(['color'], axis = 1, inplace =True)\n",
        "df_spiral2_DBScan.drop(['color'], axis = 1, inplace =True)\n",
        "df_boxes3_DBScan.drop(['color'], axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.322194Z",
          "iopub.execute_input": "2022-11-22T14:58:51.322784Z",
          "iopub.status.idle": "2022-11-22T14:58:51.333572Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.322749Z",
          "shell.execute_reply": "2022-11-22T14:58:51.332284Z"
        },
        "trusted": true,
        "id": "DZMLnvx8JswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=20, min_samples=9, metric='euclidean')\n",
        "y_DBScan = dbscan.fit_predict(df_DBScan)\n",
        "\n",
        "DBScan_blob = DBSCAN(eps=13, min_samples=45, metric='euclidean')\n",
        "DBScan_dart = DBSCAN(eps=8, min_samples=4, metric='euclidean')\n",
        "DBScan_basic = DBSCAN(eps=15, min_samples=4, metric='euclidean')\n",
        "DBScan_outliers = DBSCAN(eps=20, min_samples=4, metric='euclidean')\n",
        "DBScan_spiral2 = DBSCAN(eps=5.7, min_samples=4, metric='euclidean')\n",
        "DBScan_boxes3 = DBSCAN(eps=6, min_samples=4, metric='euclidean')\n",
        "\n",
        "y_DBScan_blob = DBScan_blob.fit_predict(df_blob_DBScan)\n",
        "y_DBScan_dart = DBScan_dart.fit_predict(df_dart_DBScan)\n",
        "y_DBScan_basic = DBScan_basic.fit_predict(df_basic2_DBScan)\n",
        "y_DBScan_outliers = DBScan_outliers.fit_predict(df_outliers_DBScan)\n",
        "y_DBScan_spiral2 = DBScan_spiral2.fit_predict(df_spiral2_DBScan)\n",
        "y_DBScan_boxes3 = DBScan_boxes3.fit_predict(df_boxes3_DBScan)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.33524Z",
          "iopub.execute_input": "2022-11-22T14:58:51.335629Z",
          "iopub.status.idle": "2022-11-22T14:58:51.923058Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.335596Z",
          "shell.execute_reply": "2022-11-22T14:58:51.921888Z"
        },
        "trusted": true,
        "id": "M3k6zPqkJswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'Cluster' columns in data sets\n",
        "df_blob_DBScan['Cluster'] = y_DBScan_blob\n",
        "df_dart_DBScan['Cluster'] = y_DBScan_dart\n",
        "df_basic2_DBScan['Cluster'] = y_DBScan_basic\n",
        "df_outliers_DBScan['Cluster'] = y_DBScan_outliers\n",
        "df_spiral2_DBScan['Cluster'] = y_DBScan_spiral2\n",
        "df_boxes3_DBScan['Cluster'] = y_DBScan_boxes3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.924757Z",
          "iopub.execute_input": "2022-11-22T14:58:51.92511Z",
          "iopub.status.idle": "2022-11-22T14:58:51.93431Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.925077Z",
          "shell.execute_reply": "2022-11-22T14:58:51.933007Z"
        },
        "trusted": true,
        "id": "_-uTA-eMJswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5 Comparing results"
      ],
      "metadata": {
        "id": "ieT-V-RSJswV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=6, ncols=2,figsize=(10,30))\n",
        "fig.suptitle('ANSWER vs DBSCAN clustering\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Answer Blob\");\n",
        "axes[0,1].scatter(df_blob_DBScan['x'], df_blob_DBScan['y'], c=df_blob_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"DBSCAN clustering Blob\");\n",
        "\n",
        "axes[1,0].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Answer Dart\");\n",
        "axes[1,1].scatter(df_dart_DBScan['x'], df_dart_DBScan['y'], c=df_dart_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"DBSCAN clustering Dart\");\n",
        "\n",
        "axes[2,0].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[2,0].set_title(\"Answer Basic\");\n",
        "axes[2,1].scatter(df_basic2_DBScan['x'], df_basic2_DBScan['y'], c=df_basic2_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,1].set_title(\"DBSCAN clustering Basic\");\n",
        "\n",
        "axes[3,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[3,0].set_title(\"Answer Outliers\");\n",
        "axes[3,1].scatter(df_outliers_DBScan['x'], df_outliers_DBScan['y'], c=df_outliers_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,1].set_title(\"DBSCAN clustering Outliers\");\n",
        "\n",
        "axes[4,0].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[4,0].set_title(\"Answer Spiral\");\n",
        "axes[4,1].scatter(df_spiral2_DBScan['x'], df_spiral2_DBScan['y'], c=df_spiral2_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,1].set_title(\"DBSCAN clustering Spiral\");\n",
        "\n",
        "axes[5,0].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[5,0].set_title(\"Answer Boxes\");\n",
        "axes[5,1].scatter(df_boxes3_DBScan['x'], df_boxes3_DBScan['y'], c=df_boxes3_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,1].set_title(\"DBSCAN clustering Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:51.938477Z",
          "iopub.execute_input": "2022-11-22T14:58:51.938923Z",
          "iopub.status.idle": "2022-11-22T14:58:56.580605Z",
          "shell.execute_reply.started": "2022-11-22T14:58:51.938875Z",
          "shell.execute_reply": "2022-11-22T14:58:56.579372Z"
        },
        "trusted": true,
        "id": "vL7sDn2vJswV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6 DBSCAN clustering model on online retail data"
      ],
      "metadata": {
        "id": "C7udfm0MJswW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
        "df_DBScan = df2.copy()\n",
        "# Checking number of items in clusters and creating 'Cluster' column\n",
        "df_DBScan['Cluster'] = y_DBScan\n",
        "df_DBScan['Cluster'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:56.582142Z",
          "iopub.execute_input": "2022-11-22T14:58:56.582945Z",
          "iopub.status.idle": "2022-11-22T14:58:56.594974Z",
          "shell.execute_reply.started": "2022-11-22T14:58:56.582902Z",
          "shell.execute_reply": "2022-11-22T14:58:56.593418Z"
        },
        "trusted": true,
        "id": "2R4qajixJswW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(data=df_DBScan, x='Amount', y='Frequency', hue = 'Cluster', s=15, palette=\"Set3\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:56.596288Z",
          "iopub.execute_input": "2022-11-22T14:58:56.596608Z",
          "iopub.status.idle": "2022-11-22T14:58:57.072014Z",
          "shell.execute_reply.started": "2022-11-22T14:58:56.59658Z",
          "shell.execute_reply": "2022-11-22T14:58:57.070785Z"
        },
        "trusted": true,
        "id": "IOYBSjijJswW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Gaussian Mixture Models (GMM)\n",
        "A Gaussian mixture model (GMM) attempts to find a mixture of multi-dimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as k-means, but because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments.\n",
        "\n",
        "The Gaussian mixture model uses multiple Gaussian distributions to fit arbitrarily shaped data.\n",
        "\n",
        "There are several single Gaussian models that act as hidden layers in this hybrid model. So the model calculates the probability that a data point belongs to a specific Gaussian distribution and that's the cluster it will fall under.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oGI-670aJswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![gausian.png](attachment:dfef474d-5414-482e-97da-ed7b45fb3446.png)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:57.073498Z",
          "iopub.execute_input": "2022-11-22T14:58:57.073953Z",
          "iopub.status.idle": "2022-11-22T14:58:57.080765Z",
          "shell.execute_reply.started": "2022-11-22T14:58:57.07392Z",
          "shell.execute_reply": "2022-11-22T14:58:57.079769Z"
        },
        "id": "NnqqwSxlJswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Advantages and Disadvantages of Gaussian Mixture Models"
      ],
      "metadata": {
        "id": "dzNDhlvWJswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Gaussian Mixture Models (GMM):**\n",
        "1. **Probabilistic estimates of belonging to each cluster** - models provide estimates of the probability that each data point belongs to each cluster. These probability estimates can be very useful when examining ambiguous data points that fall at the border of two clusters.\n",
        "\n",
        "2. **Does not assume spherical clusters** - model do not assume that all clusters are uniformly shaped spheres. Instead, gaussian mixture models can be used to accommodate clusters of varying shapes.\n",
        "\n",
        "3. **Handles clusters of differing sizes** gaussian mixture models can also be used to accommodate clusters of varying sizes.\n",
        "\n",
        "4. **Less sensitive to scale** - that means that you may not need to rescale your variables before using them for clustering.\n",
        "\n",
        "5. **Accommodates mixed membership** - in kmeans, a point belongs to one and only one cluster, whereas in GMM a point belongs to each cluster to a different degree. The degree is based on the probability of the point being generated from each clusterâ€™s (multivariate) normal distribution, with cluster center as the distributionâ€™s mean and cluster covariance as its covariance. Depending on the task, mixed membership may be more appropriate (e.g. news articles can belong to multiple topic clusters) or not (e.g. organisms can belong to only one species).\n",
        "\n",
        "\n",
        "**Disadvantages of Gaussian Mixture Model (GMM):**\n",
        "1. **Difficult to incorporate categorical features** - models operate under the assumption that all of your features are normally distributed, so they are not easily adapted to categorical data.\n",
        "\n",
        "2. **Assumes a normal distribution for features** - this means that you should take some time to look at the distributions of features before reaching for this clustering algorithm.\n",
        "\n",
        "3. **Make some assumptions about cluster shape** - this means that gaussian mixture models will not perform as well in cases where clusters are very irregularly shaped.\n",
        "\n",
        "4. **Needs sufficient data for each cluster** - you should make sure that you have enough data points in each cluster to adequately estimate the covariance. The amount of data required is not huge, but it is larger than simple algorithms that do not estimate a covariance matrix.\n",
        "\n",
        "5. **Need to specify number of clusters** - since gaussian mixture models operate under the assumption that your features are normally distributed, they can be thrown off by cases where there are many outliers in the data. Some implementations of gaussian mixture models allow for outliers to be separated out into a separate cluster.\n",
        "\n",
        "6. **Sensitive to initialization conditions** - Gsuch as the seed that is used and the starting points that are used for cluster centers. This means you may get different results if you run the algorithm multiple times.\n",
        "\n",
        "7. **Slow** - this is especially true when there are many features in your dataset."
      ],
      "metadata": {
        "id": "PrXwe1PIJswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Variations of GMM"
      ],
      "metadata": {
        "id": "LRneYv-KJswX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Variational Bayesian Gaussian mixture** - avoids the specification of the number of components for a Gaussian mixture model."
      ],
      "metadata": {
        "id": "Vq2QOehKJswX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Training of GMM on the datasets"
      ],
      "metadata": {
        "id": "k0hM5K-3JswX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying data sets\n",
        "df_GMM = df3.copy()\n",
        "df_blob_GMM = blob_df.copy()\n",
        "df_dart_GMM = dart_df.copy()\n",
        "df_basic2_GMM = basic2_df.copy()\n",
        "df_outliers_GMM = outliers_df.copy()\n",
        "df_spiral2_GMM = spiral2_df.copy()\n",
        "df_boxes3_GMM = boxes3_df.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:57.082397Z",
          "iopub.execute_input": "2022-11-22T14:58:57.082751Z",
          "iopub.status.idle": "2022-11-22T14:58:57.094627Z",
          "shell.execute_reply.started": "2022-11-22T14:58:57.082695Z",
          "shell.execute_reply": "2022-11-22T14:58:57.093302Z"
        },
        "trusted": true,
        "id": "ggTPmjLtJswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_blob_GMM.drop(['color'], axis = 1, inplace =True)\n",
        "df_dart_GMM.drop(['color'], axis = 1, inplace =True)\n",
        "df_basic2_GMM.drop(['color'], axis = 1, inplace =True)\n",
        "df_outliers_GMM.drop(['color'], axis = 1, inplace =True)\n",
        "df_spiral2_GMM.drop(['color'], axis = 1, inplace =True)\n",
        "df_boxes3_GMM.drop(['color'], axis = 1, inplace =True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:57.09652Z",
          "iopub.execute_input": "2022-11-22T14:58:57.097081Z",
          "iopub.status.idle": "2022-11-22T14:58:57.109068Z",
          "shell.execute_reply.started": "2022-11-22T14:58:57.097048Z",
          "shell.execute_reply": "2022-11-22T14:58:57.107949Z"
        },
        "trusted": true,
        "id": "cCxt4PCQJswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=8)\n",
        "y_GMM = gmm.fit_predict(df_GMM)\n",
        "\n",
        "GMM_blob = GaussianMixture(n_components=4)\n",
        "GMM_dart = GaussianMixture(n_components=2)\n",
        "GMM_basic = GaussianMixture(n_components=5)\n",
        "GMM_outliers = GaussianMixture(n_components=3)\n",
        "GMM_spiral2 = GaussianMixture(n_components=2)\n",
        "GMM_boxes3 = GaussianMixture(n_components=12)\n",
        "\n",
        "y_GMM_blob = GMM_blob.fit_predict(df_blob_GMM)\n",
        "y_GMM_dart = GMM_dart.fit_predict(df_dart_GMM)\n",
        "y_GMM_basic = GMM_basic.fit_predict(df_basic2_GMM)\n",
        "y_GMM_outliers = GMM_outliers.fit_predict(df_outliers_GMM)\n",
        "y_GMM_spiral2 = GMM_spiral2.fit_predict(df_spiral2_GMM)\n",
        "y_GMM_boxes3 = GMM_boxes3.fit_predict(df_boxes3_GMM)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:57.110862Z",
          "iopub.execute_input": "2022-11-22T14:58:57.111221Z",
          "iopub.status.idle": "2022-11-22T14:58:58.611226Z",
          "shell.execute_reply.started": "2022-11-22T14:58:57.111186Z",
          "shell.execute_reply": "2022-11-22T14:58:58.609182Z"
        },
        "trusted": true,
        "id": "OFBmczUtJswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating 'Cluster' columns in data sets\n",
        "df_blob_GMM['Cluster'] = y_GMM_blob\n",
        "df_dart_GMM['Cluster'] = y_GMM_dart\n",
        "df_basic2_GMM['Cluster'] = y_GMM_basic\n",
        "df_outliers_GMM['Cluster'] = y_GMM_outliers\n",
        "df_spiral2_GMM['Cluster'] = y_GMM_spiral2\n",
        "df_boxes3_GMM['Cluster'] = y_GMM_boxes3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:58.613826Z",
          "iopub.execute_input": "2022-11-22T14:58:58.615209Z",
          "iopub.status.idle": "2022-11-22T14:58:58.638633Z",
          "shell.execute_reply.started": "2022-11-22T14:58:58.615134Z",
          "shell.execute_reply": "2022-11-22T14:58:58.636747Z"
        },
        "trusted": true,
        "id": "uI-O60YzJswX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Comparing results"
      ],
      "metadata": {
        "id": "rgZh9x3mJswX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=6, ncols=2,figsize=(10,30))\n",
        "fig.suptitle('ANSWER vs GMM clustering\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Answer Blob\");\n",
        "axes[0,1].scatter(df_blob_GMM['x'], df_blob_GMM['y'], c=df_blob_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"GMM clustering Blob\");\n",
        "\n",
        "axes[1,0].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Answer Dart\");\n",
        "axes[1,1].scatter(df_dart_GMM['x'], df_dart_GMM['y'], c=df_dart_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"GMM clustering Dart\");\n",
        "\n",
        "axes[2,0].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[2,0].set_title(\"Answer Basic\");\n",
        "axes[2,1].scatter(df_basic2_GMM['x'], df_basic2_GMM['y'], c=df_basic2_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,1].set_title(\"GMM clustering Basic\");\n",
        "\n",
        "axes[3,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[3,0].set_title(\"Answer Outliers\");\n",
        "axes[3,1].scatter(df_outliers_GMM['x'], df_outliers_GMM['y'], c=df_outliers_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,1].set_title(\"GMM clustering Outliers\");\n",
        "\n",
        "axes[4,0].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[4,0].set_title(\"Answer Spiral\");\n",
        "axes[4,1].scatter(df_spiral2_GMM['x'], df_spiral2_GMM['y'], c=df_spiral2_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,1].set_title(\"GMM clustering Spiral\");\n",
        "\n",
        "axes[5,0].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[5,0].set_title(\"Answer Boxes\");\n",
        "axes[5,1].scatter(df_boxes3_GMM['x'], df_boxes3_GMM['y'], c=df_boxes3_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,1].set_title(\"GMM clustering Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:58:58.641087Z",
          "iopub.execute_input": "2022-11-22T14:58:58.642001Z",
          "iopub.status.idle": "2022-11-22T14:59:03.152492Z",
          "shell.execute_reply.started": "2022-11-22T14:58:58.641926Z",
          "shell.execute_reply": "2022-11-22T14:59:03.150951Z"
        },
        "trusted": true,
        "id": "c1t1jDyAJswY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5 GMM clustering model on online retail data"
      ],
      "metadata": {
        "id": "-NiTeXxEJswY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We called the df, that's why we need to refer to previous df to add cluster numbers\n",
        "df_GMM = df2.copy()\n",
        "# Checking number of items in clusters and creating 'Cluster' column\n",
        "df_GMM['Cluster'] = y_GMM\n",
        "df_GMM['Cluster'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:59:03.153779Z",
          "iopub.execute_input": "2022-11-22T14:59:03.154106Z",
          "iopub.status.idle": "2022-11-22T14:59:03.163595Z",
          "shell.execute_reply.started": "2022-11-22T14:59:03.154078Z",
          "shell.execute_reply": "2022-11-22T14:59:03.162501Z"
        },
        "trusted": true,
        "id": "XAnurQ-lJswY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(data=df_GMM, x='Amount', y='Frequency', hue = 'Cluster', s=15, palette=\"Set3\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:59:03.165287Z",
          "iopub.execute_input": "2022-11-22T14:59:03.16597Z",
          "iopub.status.idle": "2022-11-22T14:59:03.823683Z",
          "shell.execute_reply.started": "2022-11-22T14:59:03.165919Z",
          "shell.execute_reply": "2022-11-22T14:59:03.822645Z"
        },
        "trusted": true,
        "id": "1NAs7-o2JswY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. All algorithm comparison"
      ],
      "metadata": {
        "id": "31uPrmkJJswZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=6, ncols=5,figsize=(30,30))\n",
        "fig.suptitle('ANSWER vs different algorithm\\n', size = 18)\n",
        "\n",
        "axes[0,0].scatter(blob_df['x'], blob_df['y'], c=blob_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[0,0].set_title(\"Answer Blob\");\n",
        "axes[0,1].scatter(df_blob_GMM['x'], df_blob_GMM['y'], c=df_blob_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,1].set_title(\"GMM clustering Blob\");\n",
        "axes[0,2].scatter(df_blob_kmeans['x'], df_blob_kmeans['y'], c=df_blob_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,2].set_title(\"K-Means Blob\");\n",
        "axes[0,3].scatter(df_blob_AgglomerativeC['x'], df_blob_AgglomerativeC['y'], c=df_blob_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,3].set_title(\"Hierarchical clustering Blob\");\n",
        "axes[0,4].scatter(df_blob_DBScan['x'], df_blob_DBScan['y'], c=df_blob_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[0,4].set_title(\"DBSCAN clustering Blob\");\n",
        "\n",
        "axes[1,0].scatter(dart_df['x'], dart_df['y'], c=dart_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[1,0].set_title(\"Answer Dart\");\n",
        "axes[1,1].scatter(df_dart_GMM['x'], df_dart_GMM['y'], c=df_dart_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,1].set_title(\"GMM clustering Dart\");\n",
        "axes[1,2].scatter(df_dart_kmeans['x'], df_dart_kmeans['y'], c=df_dart_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,2].set_title(\"K-Means Dart\");\n",
        "axes[1,3].scatter(df_dart_AgglomerativeC['x'], df_dart_AgglomerativeC['y'], c=df_dart_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,3].set_title(\"Hierarchical clustering Dart\");\n",
        "axes[1,4].scatter(df_dart_DBScan['x'], df_dart_DBScan['y'], c=df_dart_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[1,4].set_title(\"DBSCAN clustering Dart\");\n",
        "\n",
        "axes[2,0].scatter(basic2_df['x'], basic2_df['y'], c=basic2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[2,0].set_title(\"Answer Basic\");\n",
        "axes[2,1].scatter(df_basic2_GMM['x'], df_basic2_GMM['y'], c=df_basic2_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,1].set_title(\"GMM clustering Basic\");\n",
        "axes[2,2].scatter(df_basic_kmeans['x'], df_basic_kmeans['y'], c=df_basic_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,2].set_title(\"K-Means Basic\");\n",
        "axes[2,3].scatter(df_basic2_AgglomerativeC['x'], df_basic2_AgglomerativeC['y'], c=df_basic2_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,3].set_title(\"Hierarchical clustering Basic\");\n",
        "axes[2,4].scatter(df_basic2_DBScan['x'], df_basic2_DBScan['y'], c=df_basic2_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[2,4].set_title(\"DBSCAN clustering Basic\");\n",
        "\n",
        "axes[3,0].scatter(outliers_df['x'], outliers_df['y'], c=outliers_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[3,0].set_title(\"Answer Outliers\");\n",
        "axes[3,1].scatter(df_outliers_GMM['x'], df_outliers_GMM['y'], c=df_outliers_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,1].set_title(\"GMM clustering Outliers\");\n",
        "axes[3,2].scatter(df_outliers_kmeans['x'], df_outliers_kmeans['y'], c=df_outliers_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,2].set_title(\"K-Means Outliers\");\n",
        "axes[3,3].scatter(df_outliers_AgglomerativeC['x'], df_outliers_AgglomerativeC['y'], c=df_outliers_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,3].set_title(\"Hierarchical clustering Outliers\");\n",
        "axes[3,4].scatter(df_outliers_DBScan['x'], df_outliers_DBScan['y'], c=df_outliers_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[3,4].set_title(\"DBSCAN clustering Outliers\");\n",
        "\n",
        "axes[4,0].scatter(spiral2_df['x'], spiral2_df['y'], c=spiral2_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[4,0].set_title(\"Answer Spiral\");\n",
        "axes[4,1].scatter(df_spiral2_GMM['x'], df_spiral2_GMM['y'], c=df_spiral2_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,1].set_title(\"GMM clustering Spiral\");\n",
        "axes[4,2].scatter(df_spiral2_kmeans['x'], df_spiral2_kmeans['y'], c=df_spiral2_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,2].set_title(\"K-Means Spiral\");\n",
        "axes[4,3].scatter(df_spiral2_AgglomerativeC['x'], df_spiral2_AgglomerativeC['y'], c=df_spiral2_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,3].set_title(\"Hierarchical clustering Spiral\");\n",
        "axes[4,4].scatter(df_spiral2_DBScan['x'], df_spiral2_DBScan['y'], c=df_spiral2_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[4,4].set_title(\"DBSCAN clustering Spiral\");\n",
        "\n",
        "axes[5,0].scatter(boxes3_df['x'], boxes3_df['y'], c=boxes3_df['color'], s=10, cmap = \"Set3\")\n",
        "axes[5,0].set_title(\"Answer Boxes\");\n",
        "axes[5,1].scatter(df_boxes3_GMM['x'], df_boxes3_GMM['y'], c=df_boxes3_GMM['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,1].set_title(\"GMM clustering Boxes\");\n",
        "axes[5,2].scatter(df_boxes3_kmeans['x'], df_boxes3_kmeans['y'], c=df_boxes3_kmeans['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,2].set_title(\"K-Means Boxes\");\n",
        "axes[5,3].scatter(df_boxes3_AgglomerativeC['x'], df_boxes3_AgglomerativeC['y'], c=df_boxes3_AgglomerativeC['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,3].set_title(\"Hierarchical clustering Boxes\");\n",
        "axes[5,4].scatter(df_boxes3_DBScan['x'], df_boxes3_DBScan['y'], c=df_boxes3_DBScan['Cluster'], s=10, cmap = \"Set3\")\n",
        "axes[5,4].set_title(\"DBSCAN clustering Boxes\");\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-22T14:59:03.825126Z",
          "iopub.execute_input": "2022-11-22T14:59:03.82544Z",
          "iopub.status.idle": "2022-11-22T14:59:15.357603Z",
          "shell.execute_reply.started": "2022-11-22T14:59:03.825412Z",
          "shell.execute_reply": "2022-11-22T14:59:15.356785Z"
        },
        "_kg_hide-input": true,
        "trusted": true,
        "id": "FI68THNZJswZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some referrals\n",
        "\n",
        "https://crunchingthedata.com/when-to-use-dbscan/\n",
        "\n",
        "https://crunchingthedata.com/when-to-use-hierarchical-clustering/\n",
        "\n",
        "https://crunchingthedata.com/when-to-use-gaussian-mixture-models/\n",
        "\n",
        "https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/"
      ],
      "metadata": {
        "id": "hneEP935JswZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SAByxH1IJswa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}